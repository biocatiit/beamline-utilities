import os.path
import re
import json
import glob
import argparse

import numpy as np

def parseBioCATlogfile(filename):
    datadir, fname = os.path.split(filename)

    countFilename=os.path.join(datadir, '_'.join(fname.split('_')[:-1])+'.log')

    with open(countFilename,'rU') as f:
        allLines=f.readlines()

    searchName='.'.join(fname.split('.')[:-1])

    line_num=0

    counters = {}

    for i, line in enumerate(allLines):
        if line.startswith('#'):
            if line.startswith('#Filename') or line.startswith('#image'):
                labels = line.strip('#').split('\t')
                offset = i
            else:
                key = line.strip('#').split(':')[0]
                val = ':'.join(line.strip('#').split(':')[1:])
                counters[key] = val
        else:
            break

    test_idx = int(searchName.split('_')[-1]) + offset

    if searchName in allLines[test_idx]:
        line_num = test_idx
    else:
        for a in range(1,len(allLines)):
            if searchName in allLines[a]:
                line_num=a

    if line_num>0:
        vals=allLines[line_num].split('\t')

        for a in range(len(labels)):
            counters[labels[a]] = vals[a]

    return counters

def loadPrimusDatFile(filename):
    ''' Loads a Primus .dat format file '''

    iq_pattern = re.compile('\s*\d*[.]\d*[+eE-]*\d+\s+-?\d*[.]\d*[+eE-]*\d+\s+\d*[.]\d*[+eE-]*\d+\s*')

    i = []
    q = []
    err = []

    with open(filename, 'rU') as f:
        lines = f.readlines()

    if len(lines) == 0:
        return

    comment = ''
    line = lines[0]
    j=0
    while line.split() and line.split()[0].strip()[0] == '#':
        comment = comment+line
        j = j+1
        line = lines[j]

    fileHeader = {'comment':comment}
    parameters = {'filename' : os.path.split(filename)[1],
                  'counters' : fileHeader}

    if comment.find('model_intensity') > -1:
        #FoXS file with a fit! has four data columns
        is_foxs_fit=True
        imodel = []
    else:
        is_foxs_fit = False

    for line in lines:
        iq_match = iq_pattern.match(line)

        if iq_match:
            if not is_foxs_fit:
                found = iq_match.group().split()
                q.append(float(found[0]))
                i.append(float(found[1]))
                err.append(float(found[2]))
            else:
                found = line.split()
                q.append(float(found[0]))
                i.append(float(found[1]))
                imodel.append(float(found[2]))
                err.append(float(found[3]))


    #Check to see if there is any header from RAW, and if so get that.
    header = []
    for j in range(len(lines)):
        if '### HEADER:' in lines[j]:
            header = lines[j+1:]

    hdict = None

    if len(header)>0:
        hdr_str = ''
        for each_line in header:
            hdr_str=hdr_str+each_line
        try:
            hdict = dict(json.loads(hdr_str))
        except Exception:
            # print 'Unable to load header/analysis information. Maybe the file was not generated by RAW or was generated by an old version of RAW?'
            hdict = {}


    i = np.array(i)
    q = np.array(q)
    err = np.array(err)

    if hdict:
        for each in hdict.iterkeys():
            if each != 'filename':
                parameters[each] = hdict[each]

    return q, i, err, parameters

def writeHeader(d, f2, ignore_list = []):
    f2.write('### HEADER:\n\n')

    ignore_list.append('fit_sasm')
    ignore_list.append('orig_sasm')

    for ignored_key in ignore_list:
        if ignored_key in d.keys():
            del d[ignored_key]

    f2.write(json.dumps(d, indent = 4, sort_keys = True, cls = MyEncoder))

    f2.write('\n\n')


#This class goes with write header, and was lifted from:
#https://stackoverflow.com/questions/27050108/convert-numpy-type-to-python/27050186#27050186
class MyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(MyEncoder, self).default(obj)

def writeDatFile(q, i, err, header, filename):
    ''' Writes an ASCII file from a measurement object, using the RAD format '''
    with open(filename, 'w') as f2:
        f2.write('### DATA:\n\n')
        f2.write('         Q               I              Error\n')
        f2.write('%d\n' % len(q))

        for idx in range(len(q)):
            line = ('%.8E %.8E %.8E\n') % ( q[idx], i[idx], err[idx])
            f2.write(line)

        f2.write('\n')

        f2.write('\n')
        writeHeader(header, f2)

def load_and_normalize_dat(filename, source_dir, log_dir, norm_by='I1'):
    q, i, err, params = loadPrimusDatFile(filename)
    counters = parseBioCATlogfile(filename.replace(source_dir, log_dir))

    params['counters'] = counters

    i = i/float(counters[norm_by])
    err = err/float(counters[norm_by])

    return q, i, err, params

def normalize_dats(source_dir, fprefix, output_dir, log_dir):
    flist = glob.glob(os.path.join(source_dir, '{}*.dat'.format(fprefix)))
    flist.sort(key=lambda x: (int(x.split('_')[-2]),
        int(x.split('_')[-1].split('.')[0])))

    for fname in flist:
        print(fname)
        q, i, err, params = load_and_normalize_dat(fname, source_dir, log_dir)

        outname = os.path.join(output_dir, os.path.basename(fname))

        writeDatFile(q, i, err, params, outname)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Normalize .dat files')
    parser.add_argument('source_dir', help='The directory with the files')
    parser.add_argument('file_prefix',help='The file prefix to process')
    parser.add_argument('-o', '--output-dir', metavar='DIR', dest='output_dir', help='The output directory for normalized files (default: source_dir)')
    parser.add_argument('-l', '--log-dir', metavar='DIR', dest='log_dir', help='The directory of the log file (default: source directory)')
    args = parser.parse_args()

    source_dir = args.source_dir
    fprefix = args.file_prefix

    if args.output_dir is not None:
        output_dir = args.output_dir
    else:
        output_dir = source_dir

    if args.log_dir is not None:
        log_dir = args.log_dir
    else:
        log_dir = source_dir

    normalize_dats(source_dir, fprefix, output_dir, log_dir)
